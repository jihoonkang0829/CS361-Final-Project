{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Generation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport numpy.linalg as la\nimport matplotlib.pyplot as plt\nimport torch\n\ndim_theta = 10\ndata_num = 1000\nscale = .1\n\ntheta_true = np.ones((dim_theta,1))\nprint('True theta:', theta_true.reshape(-1))\n\nA = np.random.uniform(low=-1.0, high=1.0, size=(data_num,dim_theta))\ny_data = np.matmul(A,theta_true)+np.random.normal(loc=0.0, scale=scale, size=(data_num,1))\n\nA_test = np.random.uniform(low=-1.0, high=1.0, size=(50,dim_theta))\ny_test = np.matmul(A_test,theta_true)+np.random.normal(loc=0.0, scale=scale, size=(50,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Solving for the exact mean squared loss (solving Ax = b)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(y_data)\n#print(A.size)\n#print(y_data.size)\n\n'''\nHints:\n1. Use np.matmul() and la.inv() to solve for x in Ax = b.\n2. Use the defined variable A in Ax = b. Use y_data as b. Use theta_pred as x.\n'''\nx,residuals,rank,s = la.lstsq(A, y_data, rcond=1)\ntheta_pred = x\n\nprint('Empirical theta', theta_pred.reshape(-1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SGD Variants Noisy Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1\nmax_iter = 1000\nlr = 0.001\ntheta_init = np.random.random((10,1)) * 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def noisy_val_grad(theta_hat, data_, label_, deg_=2.):\n    gradient = np.zeros_like(theta_hat)\n    loss = 0\n    \n    for i in range(data_.shape[0]):\n        x_ = data_[i, :].reshape(-1,1)\n        y_ = label_[i, 0]\n        err = np.sum(x_ * theta_hat) - y_\n\n        '''\n        Hints:\n        1. Find the gradient and loss for each data point x_.\n        2. For grad, you need err, deg_, and x_.\n        3. For l, you need err and deg_ only.\n        '''\n        grad = deg_ * np.sign(err) * np.abs(err) ** (deg_ - 1) * x_\n        l = np.abs(err) ** deg_\n        \n        loss += l / data_.shape[0]\n        gradient += grad / data_.shape[0]\n        \n    return loss, gradient","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running SGD Variants","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"deg_ = 2.\nnum_rep = 10\nmax_iter = 1000\nfig, ax = plt.subplots(figsize=(10,10))\nbest_vals = dict()\ntest_exp_interval = 50\ngrad_artificial_normal_noise_scale = 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"la_ada = 0.1\nlr = 0.001\ndeg_ = 5\n\nfor method_idx, method in enumerate(['adam', 'sgd', 'adagrad']):\n    test_loss_mat = []\n    train_loss_mat = []\n    \n    for replicate in range(num_rep):\n        if replicate % 20 == 0:\n            print(method, replicate)\n            \n        if method == 'adam':\n            beta_1 = 0.9\n            beta_2 = 0.999\n            m = 0\n            v = 0\n            epsilon = 1e-8\n\n        if method == 'adagrad':\n            epsilon = 1e-8\n            squared_sum = 0\n            \n        theta_hat = theta_init.copy()\n        test_loss_list = []\n        train_loss_list = []\n        \n        for t in range(max_iter):\n            idx = np.random.choice(data_num,batch_size)\n            train_loss, gradient = noisy_val_grad(theta_hat, A[idx,:], y_data[idx,:], deg_=deg_)\n            artificial_grad_noise = np.random.randn(10,1) * grad_artificial_normal_noise_scale + np.sign(np.random.random((10,1))-0.5) * 0.\n            gradient = gradient + artificial_grad_noise\n            train_loss_list.append(train_loss)\n            \n            if t % test_exp_interval == 0:\n                test_loss, _ = noisy_val_grad(theta_hat, A_test[:,:], y_test[:,:], deg_=deg_)\n                test_loss_list.append(test_loss)                \n            \n            if method == 'adam':\n                m = beta_1 * m + (1 - beta_1) * gradient\n                v = beta_2 * v + (1 - beta_2) * gradient ** 2\n                m_hat = m / (1 - beta_1 ** (t + 1))\n                v_hat = v / (1 - beta_2 ** (t + 1))\n                theta_hat = theta_hat - lr * m_hat / (np.sqrt(v_hat) + epsilon)\n            \n            elif method == 'adagrad':\n                if la_ada == -1: la_ada = lr\n                squared_sum = squared_sum + gradient ** 2\n                theta_hat = theta_hat - lr * gradient / (np.sqrt(squared_sum + epsilon))\n            \n            elif method == 'sgd':\n                theta_hat = theta_hat - lr * gradient\n        \n        test_loss_mat.append(test_loss_list)\n        train_loss_mat.append(train_loss_list)\n        \n    print(method, 'done')\n    x_axis = np.arange(max_iter)[::test_exp_interval]\n    \n    print('test_loss_np is a 2d array with num_rep rows and each column denotes a specific update stage in training')\n    print('The elements of test_loss_np are the test loss values computed in each replicate and training stage.')\n    test_loss_np = np.array(test_loss_mat)\n    \n    '''\n    Hints:\n    1. Use test_loss_np in np.mean() with axis = 0\n    '''\n    test_loss_mean = np.sum(test_loss_np, axis = 0).reshape(-1) / np.sqrt(num_rep)\n\n    '''\n    Hints:\n    1. Use test_loss_np in np.std() with axis = 0 \n    2. Divide by np.sqrt() using num_rep as a parameter\n    '''\n    test_loss_se = np.std(test_loss_np, axis = 0).reshape(-1) / np.sqrt(num_rep)\n\n    plt.errorbar(x_axis, test_loss_mean, yerr=2.5*test_loss_se, label=method)\n    best_vals[method] = min(test_loss_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_vals = {k: int(v*1000)/1000. for k,v in best_vals.items()} # A weird way to round numbers\nplt.title(f'Test Loss \\n(objective degree: {deg_},  best values: {best_vals})')\nplt.ylabel('Test Loss')\nplt.legend()\nplt.xlabel('Updates')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}